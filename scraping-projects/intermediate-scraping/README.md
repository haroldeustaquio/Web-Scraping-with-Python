# Intermediate Scraping

Welcome to the **Intermediate Scraping** section of this repository. In this section, you'll find projects that delve deeper into web scraping, focusing on handling dynamic content, multiple pages, and more complex web scraping scenarios. The goal is to expand your skills and knowledge in web scraping using Python.

## ðŸš€ Tools and Libraries

In this section, I will utilize the following tools:

1. **`Selenium`**: 
   - A powerful tool for automating web browsers. It allows you to interact with dynamic content and simulate user behavior.
   - [Documentation](https://www.selenium.dev/documentation/)

2. **`Scrapy`**: 
   - A fast and powerful web scraping framework that enables you to write spiders for crawling and scraping data from websites.
   - [Documentation](https://docs.scrapy.org/en/latest/)

3. **`regex`**: 
   - A library for string matching and manipulation, useful for extracting specific data patterns from HTML content.
   - [Documentation](https://docs.python.org/3/library/re.html)

These tools will help you scrape more complex websites that utilize AJAX, pagination, and other dynamic content strategies.

## ðŸ“‚ Project Structure

This directory contains the following projects:

### 1. Dynamic Content Scraper
- **Goal**: Learn to scrape data from websites that load content dynamically using JavaScript (AJAX).
- **Tools Used**: `Selenium`, `BeautifulSoup`, `pandas`.
- **Project Path**: `/intermediate-scraping/project-dynamic-content/`

### 2. Multipage Scraper
- **Goal**: Learn to navigate through multiple pages of a website, such as paginated search results.
- **Tools Used**: `Scrapy`, `BeautifulSoup`, `pandas`.
- **Project Path**: `/intermediate-scraping/project-multiple-pages/`

Each project has its own directory with a detailed README explaining how the scraper works and how to run the code.

